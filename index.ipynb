{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d295f8f5-f583-4764-86cd-90cabd44eb59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/alex/Documents/gpt4all/models/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/alex/Documents/gpt4all/models/wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7477.73 MB (+ 1600.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/Users/alex/Documents/gpt4all/lib/python3.9/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x10e60f7e0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x10e60fa30\n",
      "ggml_metal_init: loaded kernel_mul                            0x10e60fc80\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x10e6120c0\n",
      "ggml_metal_init: loaded kernel_scale                          0x10e612310\n",
      "ggml_metal_init: loaded kernel_silu                           0x10e612560\n",
      "ggml_metal_init: loaded kernel_relu                           0x10e6127b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x10e612a00\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x10e612c50\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x10e612ea0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x10e6130f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x10e613340\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x10e613590\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x10e6137e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x10e613a30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x10e613c80\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x10e613ed0\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x10e614120\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x10e614370\n",
      "ggml_metal_init: loaded kernel_norm                           0x10e6145c0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x10e614810\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x10e614a60\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x10e614cb0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x10e614f00\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x10e615150\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x10e6153a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x10e6155f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x10e615840\n",
      "ggml_metal_init: loaded kernel_rope                           0x10e615a90\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x10e615ce0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x10e615f30\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x10e616180\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x10e6163d0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 49152.00 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, (16064.92 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (16077.09 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1602.00 MB, (17679.09 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   290.00 MB, (17969.09 / 49152.00)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (18161.09 / 49152.00)\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is  \n",
      "```python\n",
      "Paris\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from os import getcwd\n",
    "from gpt4all import GPT4All\n",
    "\n",
    "# Best overall model\n",
    "# Instruction based\n",
    "# Gives very long responses\n",
    "# Finetuned with only 1k of high-quality data\n",
    "# Trained by Microsoft and Peking University\n",
    "# Cannot be used commercially\n",
    "wizardlm = \"wizardlm-13b-v1.1-superhot-8k.ggmlv3.q4_0.bin\"\n",
    "# Trained on uncensored assistant data and instruction data\n",
    "# Instruction based\n",
    "# Cannot be used commercially\n",
    "wizardlmUncensored = \"wizardLM-13B-Uncensored.ggmlv3.q4_0.bin\"\n",
    "# Best overall smaller model\n",
    "# Fast responses\n",
    "# Instruction based\n",
    "# Trained by TII\n",
    "# Finetuned by Nomic AI\n",
    "# Licensed for commercial use\n",
    "falcon = \"ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "# New model with novel dataset\n",
    "# Instruction based\n",
    "# Explain tuned datasets\n",
    "# Orca Research Paper dataset construction approaches\n",
    "# Licensed for commercial use\n",
    "orca7b = \"orca-mini-7b.ggmlv3.q4_0.bin\"\n",
    "# Largest version of new model with novel dataset\n",
    "# Instruction based\n",
    "# Explain tuned datasets\n",
    "# Orca Research Paper dataset construction approaches\n",
    "# Licensed for commercial use\n",
    "orca13b = \"orca-mini-13b.ggmlv3.q4_0.bin\"\n",
    "\n",
    "model = GPT4All(getcwd() + \"/models/\" + wizardlm)\n",
    "\n",
    "def prompt(prompt, max_tokens=32):\n",
    "    print(prompt, model.generate(prompt, max_tokens=max_tokens))\n",
    "\n",
    "prompt(\"The capital of France is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c995938b-d8bc-48da-b98e-94e5ee01ae05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Who is Pepe?\n",
      " Pepe the Frog, also known as Kermit's best friend and sidekick. He was originally created by Matt Furie in 2\n"
     ]
    }
   ],
   "source": [
    "prompt(\"\"\"\n",
    "Who is Pepe?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e8b96bb-f229-46c9-bc14-c5ab6abb7c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What is \"The Matrix\".\n",
      " \" and what does it represent?\n"
     ]
    }
   ],
   "source": [
    "prompt(\"\"\"\n",
    "What is \"The Matrix\".\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1479074-4783-4b84-8529-59f60eeb8411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Why people barf?\n",
      " ```vbnet\n",
      "// Why do people vomit? ü§¢\n",
      "People may vomit for various reasons, including:\n",
      "\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "prompt(\"\"\"\n",
    "Why people barf?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f42152d2-7796-4285-b73e-f0e1a3aad529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ü–æ—á–µ–º—É –ª—é–¥–µ–π —Ç–æ—à–Ω–∏—Ç?\n",
      " –¢–æ–Ω–∏—Ç ‚Äì —ç—Ç–æ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞ –∫–æ–∂–µ –ø–æ—è–≤–ª—è—é—Ç—Å—è –∫—Ä–∞—Å–Ω—ã–µ –∏–ª–∏ —Ä–æ–∑–æ–≤—ã–µ –ø—è—Ç–Ω–∞ —Å–≤–µ—Ä\n"
     ]
    }
   ],
   "source": [
    "prompt(\"\"\"\n",
    "–ü–æ—á–µ–º—É –ª—é–¥–µ–π —Ç–æ—à–Ω–∏—Ç?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d2122b1-2db0-45d0-b156-cbef52380a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–¢urk√ße biliyur musun?\n",
      " \n",
      "T√ºrk√ßeyi ≈üikayet olarak kullanmaktadƒ±r. Bu, s√∂zc√ºƒü\n"
     ]
    }
   ],
   "source": [
    "prompt(\"\"\"\n",
    "–¢urk√ße biliyur musun?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096f4e7-e7c5-49f8-95fe-99025e6ec44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
